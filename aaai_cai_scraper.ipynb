{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np \n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping papers of 2023 and 2024 from the archive site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaai_papers = {\n",
    "        \"Year\" : [], \n",
    "        \"Title\" : [], \n",
    "        \"Authors\" : [],\n",
    "        \"Number of authors\" : [],\n",
    "        \"Abstract\" : [],\n",
    "        \"Citations\" : [],\n",
    "        \"Keyword\" : [],\n",
    "        \"Paper file\" : []\n",
    "        \n",
    "}\n",
    "\n",
    "original_url = \"https://ojs.aaai.org/index.php/AAAI/issue/archive\"\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "content = requests.get(original_url,headers=headers).text\n",
    "\n",
    "scraper = BeautifulSoup(content, parser=\"html.parser\")\n",
    "\n",
    "tracks = scraper.find(\"ul\", class_=\"issues_archive\")\n",
    "\n",
    "track_links = tracks.find_all(\"li\")\n",
    "i=0\n",
    "\n",
    "for  li in track_links:\n",
    "    papers_link = li.find('h2').find('a').get(\"href\")\n",
    "\n",
    "    track_title = li.find('h2').find('a').text\n",
    "\n",
    "    if \"24\" in track_title:\n",
    "        year = \"2024\"\n",
    "    elif \"23\" in track_title:\n",
    "        year =\"2023\"\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "    papers_page = requests.get(papers_link,headers=headers).text\n",
    "\n",
    "    scraper = BeautifulSoup(papers_page, parser=\"html.parser\")\n",
    "\n",
    "    div_tracks = scraper.find('div', class_='sections')\n",
    "    div_sections = scraper.find_all('div', class_='section')\n",
    "\n",
    "    for section in div_sections :\n",
    "        keyword = section.find('h2').text\n",
    "        \n",
    "        papers = section.find('ul').find_all('li', recursive=False)\n",
    "        for paper in papers:\n",
    "\n",
    "            print(\"year: \",year,\" \",i)\n",
    "            i=i+1\n",
    "\n",
    "            paper_title=paper.find('h3').find('a').text\n",
    "           \n",
    "            paper_link=paper.find('h3').find('a').get(\"href\")\n",
    "            authors=paper.find('div', class_=\"authors\").text\n",
    "            authors = authors.lstrip()\n",
    "            pdf_link = paper.find_all('li')[0].find('a').get(\"href\")\n",
    "\n",
    "            paper_page = requests.get(paper_link,headers=headers).text\n",
    "            scraper = BeautifulSoup(paper_page, parser=\"html.parser\")\n",
    "\n",
    "            abstract= scraper.find('section', class_='item abstract')\n",
    "            abstract.find('h2').extract()\n",
    "            abstract = abstract.get_text(strip=True)\n",
    "            aaai_papers[\"Year\"].append(year)\n",
    "            aaai_papers[\"Title\"].append(paper_title.lstrip())\n",
    "            aaai_papers[\"Authors\"].append(authors)\n",
    "            aaai_papers[\"Number of authors\"].append(len(authors.split(\",\")))\n",
    "            aaai_papers[\"Abstract\"].append(abstract)\n",
    "            aaai_papers[\"Citations\"].append(0)  \n",
    "            aaai_papers[\"Paper file\"].append(pdf_link.lstrip())\n",
    "            aaai_papers[\"Keyword\"].append(keyword.lstrip()) \n",
    "\n",
    "papers_23_24 = pd.DataFrame(aaai_papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping papers of years from 2022 to 1980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##scraping one year\n",
    "def scrape_year(a):\n",
    "    \n",
    "    aaai_papers = {\n",
    "        \"Year\" : [], \n",
    "        \"Title\" : [], \n",
    "        \"Authors\" : [],\n",
    "        \"Number of authors\" : [],\n",
    "        \"Abstract\" : [],\n",
    "        \"Citations\" : [],\n",
    "        \"Keyword\" : [],\n",
    "        \"Paper file\" : []\n",
    "        \n",
    "    }\n",
    "    \n",
    "    i=0\n",
    "    year = a.text\n",
    "    year_link = a.get('href')\n",
    "    headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "    content_year_page = requests.get(year_link,headers=headers).text\n",
    "    scraper = BeautifulSoup(content_year_page, parser=\"html.parser\")\n",
    "\n",
    "    div_element = scraper.find('div', class_='content-sidebar-wrap')\n",
    "    li_elements = div_element.find_all('li')\n",
    "    for li in li_elements:\n",
    "        a_elements = li.find('a')\n",
    "        track_link = a_elements.get('href')\n",
    "        track_page = requests.get(track_link,headers=headers).text\n",
    "        scraper = BeautifulSoup(track_page, parser=\"html.parser\")\n",
    "\n",
    "        div_tracks = scraper.find_all('div', class_='track-wrap')\n",
    "        \n",
    "        for div_track in div_tracks:\n",
    "            keyword = div_track.find('h2').text\n",
    "            li_elements = div_track.find_all('li')\n",
    "            for li in li_elements:\n",
    "                paper_link = li.find('h5').find('a').get('href')\n",
    "                title = li.find('h5').find('a').text\n",
    "                authors = li.find('span').find('p').text\n",
    "                pdf_link = li.find('a', class_='wp-block-button').get('href')\n",
    "                paper_page = requests.get(paper_link,headers=headers).text\n",
    "                scraper = BeautifulSoup(paper_page, parser=\"html.parser\")\n",
    "\n",
    "                div_paper= scraper.find_all('div', class_='paper-section-wrap')\n",
    "                for div in div_paper:\n",
    "                    if (div.find('div', class_='attribute-output') is not None):\n",
    "                        abstract = div.find('div', class_='attribute-output').text\n",
    "                    break\n",
    "                \n",
    "                \n",
    "                if (div.find('div', class_='attribute-output') is None) :\n",
    "                    abstract = \" \"\n",
    "                    print(\" ************ none ********************\")\n",
    "\n",
    "                aaai_papers[\"Year\"].append(year)\n",
    "                aaai_papers[\"Title\"].append(title)\n",
    "                aaai_papers[\"Authors\"].append(authors)\n",
    "                aaai_papers[\"Number of authors\"].append(len(authors.split(\",\")))\n",
    "                aaai_papers[\"Abstract\"].append(abstract)\n",
    "                aaai_papers[\"Citations\"].append(0)  \n",
    "                aaai_papers[\"Paper file\"].append(pdf_link)\n",
    "                aaai_papers[\"Keyword\"].append(keyword)   \n",
    "                print(\"Year: \",year, \"paper: \", i ) \n",
    "                i=i+1      \n",
    "    return pd.DataFrame(aaai_papers)\n",
    "\n",
    "\n",
    "## scraping \n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "original_url = \"https://aaai.org/aaai-publications/aaai-conference-proceedings\"\n",
    "content_years_page = requests.get(original_url,headers=headers).text\n",
    "scraper = BeautifulSoup(content_years_page, parser=\"html.parser\")\n",
    "\n",
    "\n",
    "p_element = scraper.find('p', class_='link-block has-brand-color-7-color has-text-color')\n",
    "\n",
    "a_elements = p_element.find_all('a')\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=7) as executor:\n",
    "    results = list(executor.map(scrape_year, a_elements[2:]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving results into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat all the results\n",
    "papers = pd.DataFrame()\n",
    "\n",
    "papers = pd.concat(papers_23_24)\n",
    "papers = pd.concat(results)\n",
    "\n",
    "if not os.path.isdir(\"folder_to_save\"):\n",
    "    os.makedirs(\"folder_to_save\")\n",
    "\n",
    "csv_file = os.path.join(\"folder_to_save\", \"aaai_papers.csv\")\n",
    "papers.to_csv(csv_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
